
طبقة بتعمل زى توكينيز جوة النتورك
بتخلي التوكين الي معمولة من رقم لفيكتور من الارقام
فا تبقى دقيقة اكتر


embeddings is to represent the word in a dense vector while making sure that similar words are close to each other in the embedding space

--> What is a Dense Vector  :
vector representing the word

--> Embedding Space :
is where your embedded data lives..
المسافه بين نقطتين فيه بتوضح مدى تشابه الكلمتين دول ف المعنى





-->input embeddings to bert :
*positional encoding*
بيحدد مكان كل كلمة في ال جمله الواحده

*sentence encoding*
لو في اكتر من جمله بيحدد مكان الكلمات بالنسبه للجمله الواحده و مثلا علاقة كل جملة بالتانيه و اماكنهم بالنسبة لبعض

*token encoding*
تمثيل كل كلمة برقم معين


## Links
[244 - What are embedding layers in keras? - YouTube](https://www.youtube.com/watch?v=nam2zR7p7Os)

https://www.youtube.com/watch?v=5MaWmXwxFNQ&list=PLcWfeUsAys2nPgh-gYRlexc6xvscdvHqX&index=16&ab_channel=AssemblyAI