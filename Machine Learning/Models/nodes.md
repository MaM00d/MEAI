
## Activation Functions

### Step-Function/threshold 
### Linear

### Non-Linear
#### Soft-max
  $\text{Softmax}(x_{i}) = \frac{\exp(x_i)}{\sum_j \exp(x_j)}$



#### Sigmoid/Logic  11-Function
##### Equation 
$\sigma(z) = \frac{1} {1 + e^{-z}}$

## Links
-  [Softmax Activation Function || Softmax Function || Quick Explained || Developers Hutt - YouTube](https://www.youtube.com/watch?v=8ah-qhvaQqU&t=12s) 
- [Activation Functions | Deep Learning Tutorial 8 (Tensorflow Tutorial, Keras & Python) - YouTube](https://www.youtube.com/watch?v=icZItWxw7AI)
- [Activation Functions in Neural Networks [12 Types & Use Cases]](https://www.v7labs.com/blog/neural-networks-activation-functions)